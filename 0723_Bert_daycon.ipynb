{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# https://dacon.io/en/competitions/official/235744/codeshare/2861?page=1&dtype=recent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import re\r\n",
    "import json\r\n",
    "import os\r\n",
    "import tqdm\r\n",
    "\r\n",
    "from konlpy.tag import Okt\r\n",
    "\r\n",
    "import sklearn\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "\r\n",
    "from sklearn.metrics import log_loss, accuracy_score,f1_score\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
    "from transformers import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_without = pd.read_csv('./without_name/Cleaned_headlines.tsv', sep=\"\\t\")\r\n",
    "df_without"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "train, test = train_test_split(df_without, test_size=.2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train, test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#random seed 고정\r\n",
    "tf.random.set_seed(1234)\r\n",
    "np.random.seed(1234)\r\n",
    "BATCH_SIZE = 32\r\n",
    "NUM_EPOCHS = 3\r\n",
    "VALID_SPLIT = 0.2\r\n",
    "MAX_LEN=200"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',  cache_dir='bert_ckpt', do_lower_case=False)\r\n",
    "\r\n",
    "def bert_tokenizer(sent, MAX_LEN):\r\n",
    "    \r\n",
    "    encoded_dict=tokenizer.encode_plus(\r\n",
    "    text = sent, \r\n",
    "    add_special_tokens=True, \r\n",
    "    max_length=MAX_LEN, \r\n",
    "    pad_to_max_length=True, \r\n",
    "    return_attention_mask=True,\r\n",
    "    truncation = True)\r\n",
    "    \r\n",
    "    input_id=encoded_dict['input_ids']\r\n",
    "    attention_mask=encoded_dict['attention_mask']\r\n",
    "    token_type_id = encoded_dict['token_type_ids']\r\n",
    "    \r\n",
    "    return input_id, attention_mask, token_type_id\r\n",
    "\r\n",
    "input_ids =[]\r\n",
    "attention_masks =[]\r\n",
    "token_type_ids =[]\r\n",
    "train_data_labels = []\r\n",
    "\r\n",
    "# def clean_text(sent):\r\n",
    "#     # sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\r\n",
    "#     return sent_clean\r\n",
    "\r\n",
    "for train_sent, train_label in zip(train['뉴스제목'], train['주가변동']):\r\n",
    "    try:\r\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN=MAX_LEN)\r\n",
    "        \r\n",
    "        input_ids.append(input_id)\r\n",
    "        attention_masks.append(attention_mask)\r\n",
    "        token_type_ids.append(token_type_id)\r\n",
    "        #########################################\r\n",
    "        train_data_labels.append(train_label)\r\n",
    "        \r\n",
    "    except Exception as e:\r\n",
    "        print(e)\r\n",
    "        print(train_sent)\r\n",
    "        pass\r\n",
    "\r\n",
    "train_input_ids=np.array(input_ids, dtype=int)\r\n",
    "train_attention_masks=np.array(attention_masks, dtype=int)\r\n",
    "train_token_type_ids=np.array(token_type_ids, dtype=int)\r\n",
    "###########################################################\r\n",
    "train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\r\n",
    "train_labels=np.asarray(train_data_labels, dtype=np.int32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_input_ids[1])\r\n",
    "print(train_attention_masks[1])\r\n",
    "print(train_token_type_ids[1])\r\n",
    "print(tokenizer.decode(train_input_ids[1])) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class TFBertClassifier(tf.keras.Model):\r\n",
    "    def __init__(self, model_name, dir_path, num_class):\r\n",
    "        super(TFBertClassifier, self).__init__()\r\n",
    "\r\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\r\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\r\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, \r\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \r\n",
    "                                                name=\"classifier\")\r\n",
    "        \r\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\r\n",
    "        \r\n",
    "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\r\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\r\n",
    "        pooled_output = outputs[1] \r\n",
    "        pooled_output = self.dropout(pooled_output, training=training)\r\n",
    "        logits = self.classifier(pooled_output)\r\n",
    "\r\n",
    "        return logits\r\n",
    "\r\n",
    "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\r\n",
    "                                  dir_path='bert_ckpt',\r\n",
    "                                  num_class=46)\r\n",
    "\r\n",
    "# 학습 준비하기\r\n",
    "optimizer = tf.keras.optimizers.Adam(3e-5)\r\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
    "\r\n",
    "model_name = \"tf2_bert_classifier\"\r\n",
    "\r\n",
    "# overfitting을 막기 위한 ealrystop 추가\r\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=5)\r\n",
    "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\r\n",
    "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\r\n",
    "\r\n",
    "checkpoint_path = os.path.join(model_name, 'weights.h5')\r\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\r\n",
    "\r\n",
    "# Create path if exists\r\n",
    "if os.path.exists(checkpoint_dir):\r\n",
    "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\r\n",
    "else:\r\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\r\n",
    "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\r\n",
    "    \r\n",
    "cp_callback = ModelCheckpoint(\r\n",
    "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\r\n",
    "\r\n",
    "# 학습과 eval 시작\r\n",
    "history = cls_model.fit(train_inputs, train_labels, epochs=30, batch_size=32,\r\n",
    "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_ids =[]\r\n",
    "attention_masks =[]\r\n",
    "token_type_ids =[]\r\n",
    "train_data_labels = []\r\n",
    "\r\n",
    "# def clean_text(sent):\r\n",
    "#     sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\r\n",
    "#     return sent_clean\r\n",
    "\r\n",
    "for test_sent in test['뉴스제목']:\r\n",
    "    try:\r\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN=40)\r\n",
    "        \r\n",
    "        input_ids.append(input_id)\r\n",
    "        attention_masks.append(attention_mask)\r\n",
    "        token_type_ids.append(token_type_id)\r\n",
    "        #########################################\r\n",
    "       \r\n",
    "    except Exception as e:\r\n",
    "        print(e)\r\n",
    "        print(test_sent)\r\n",
    "        pass\r\n",
    "    \r\n",
    "test_input_ids=np.array(input_ids, dtype=int)\r\n",
    "test_attention_masks=np.array(attention_masks, dtype=int)\r\n",
    "test_token_type_ids=np.array(token_type_ids, dtype=int)\r\n",
    "###########################################################\r\n",
    "test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = cls_model.predict(test_inputs)\r\n",
    "results=tf.argmax(results, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_submission['주가변동']=results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_submission"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_submission.to_csv('bert_baseline.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "5bbcfba0af128e6b66c2bfbf999bccbbcb5a23b4768301f164b5fec2568e5d70"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}