{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "multi_classify_text_with_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.8 64-bit ('news': venv)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "8ebe0836e5189048e1d5707404c6ed8defd8d68c91d6ecaf4bf62742c145b5a4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n"
      ],
      "metadata": {
        "id": "Cb4espuLKJiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "SCjmX4zTCkRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "import os\r\n",
        "# import shutil\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "# import tensorflow_hub as hub\r\n",
        "# import tensorflow_text as text\r\n",
        "# from official.nlp import optimization  # to create AdamW optimizer\r\n",
        "\r\n",
        "# import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-07-28 16:16:24.703437: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a few reviews."
      ],
      "metadata": {
        "id": "HGm10A5HRGXp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "#@title Choose a BERT model to fine-tune\r\n",
        "\r\n",
        "bert_model_name = 'bert_multi_cased_L-12_H-768_A-12'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\r\n",
        "\r\n",
        "map_name_to_handle = {\r\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\r\n",
        "    'bert_en_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\r\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\r\n",
        "    'albert_en_base':\r\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\r\n",
        "    'electra_small':\r\n",
        "        'https://tfhub.dev/google/electra_small/2',\r\n",
        "    'electra_base':\r\n",
        "        'https://tfhub.dev/google/electra_base/2',\r\n",
        "    'experts_pubmed':\r\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\r\n",
        "    'experts_wiki_books':\r\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\r\n",
        "    'talking-heads_base':\r\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\r\n",
        "}\r\n",
        "\r\n",
        "map_model_to_preprocess = {\r\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'bert_en_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\r\n",
        "    'albert_en_base':\r\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\r\n",
        "    'electra_small':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'electra_base':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'experts_pubmed':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'experts_wiki_books':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "    'talking-heads_base':\r\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\r\n",
        "}\r\n",
        "\r\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\r\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\r\n",
        "\r\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\r\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\n"
          ]
        }
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8_ctG55-uTX",
        "outputId": "d85135dc-d719-449b-8d21-8b50b631ced6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0SQi-jWd_jzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "outputs": [],
      "metadata": {
        "id": "tXxYpK8ixL34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "def build_classifier_model():\r\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\r\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\r\n",
        "  encoder_inputs = preprocessing_layer(text_input)\r\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\r\n",
        "  outputs = encoder(encoder_inputs)\r\n",
        "  net = outputs['pooled_output']\r\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\r\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\r\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "outputs": [],
      "metadata": {
        "id": "aksj743St9ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "classifier_model = build_classifier_model()\r\n",
        "# bert_raw_result = classifier_model(tf.constant(text_test))\r\n",
        "# print(tf.sigmoid(bert_raw_result))"
      ],
      "outputs": [],
      "metadata": {
        "id": "mGMF8AZcB2Zy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "0EmzyHZXKIpm",
        "outputId": "0e1b8761-f871-4acd-95b5-6a4b38a59312"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\r\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "outputs": [],
      "metadata": {
        "id": "OWPOZE-L3AgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "epochs = 10\r\n",
        "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\r\n",
        "num_train_steps = steps_per_epoch * epochs\r\n",
        "num_warmup_steps = int(0.1*num_train_steps)\r\n",
        "\r\n",
        "init_lr = 3e-5\r\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=init_lr, name='Adam')\r\n",
        "\r\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\r\n",
        "                                          num_train_steps=num_train_steps,\r\n",
        "                                          num_warmup_steps=num_warmup_steps,\r\n",
        "                                          optimizer_type='adamw')"
      ],
      "outputs": [],
      "metadata": {
        "id": "P9eP2y9dbw32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "classifier_model.compile(optimizer=optimizer,\r\n",
        "                         loss=loss,\r\n",
        "                         metrics=metrics)"
      ],
      "outputs": [],
      "metadata": {
        "id": "-7GPDhR98jsD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\r\n",
        "history = classifier_model.fit(x=train_ds,\r\n",
        "                               validation_data=val_ds,\r\n",
        "                               epochs=epochs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3\n",
            "Epoch 1/10\n",
            "10525/10525 [==============================] - 2492s 236ms/step - loss: 0.6952 - binary_accuracy: 0.5002 - val_loss: 0.6986 - val_binary_accuracy: 0.5016\n",
            "Epoch 2/10\n",
            "10525/10525 [==============================] - 2476s 235ms/step - loss: 0.6959 - binary_accuracy: 0.4999 - val_loss: 0.6972 - val_binary_accuracy: 0.5016\n",
            "Epoch 3/10\n",
            "10525/10525 [==============================] - 2474s 235ms/step - loss: 0.6952 - binary_accuracy: 0.4998 - val_loss: 0.6959 - val_binary_accuracy: 0.5016\n",
            "Epoch 4/10\n",
            "10525/10525 [==============================] - 2475s 235ms/step - loss: 0.6948 - binary_accuracy: 0.4999 - val_loss: 0.6950 - val_binary_accuracy: 0.5016\n",
            "Epoch 5/10\n",
            "10525/10525 [==============================] - 2475s 235ms/step - loss: 0.6945 - binary_accuracy: 0.4999 - val_loss: 0.6944 - val_binary_accuracy: 0.5016\n",
            "Epoch 6/10\n",
            "10525/10525 [==============================] - 2476s 235ms/step - loss: 0.6943 - binary_accuracy: 0.5000 - val_loss: 0.6943 - val_binary_accuracy: 0.5015\n",
            "Epoch 7/10\n",
            "10525/10525 [==============================] - 2476s 235ms/step - loss: 0.6943 - binary_accuracy: 0.4999 - val_loss: 0.6934 - val_binary_accuracy: 0.5016\n",
            "Epoch 8/10\n",
            "10525/10525 [==============================] - 2476s 235ms/step - loss: 0.6941 - binary_accuracy: 0.4999 - val_loss: 0.6931 - val_binary_accuracy: 0.5016\n",
            "Epoch 9/10\n",
            "10525/10525 [==============================] - 2476s 235ms/step - loss: 0.6936 - binary_accuracy: 0.4999 - val_loss: 0.6930 - val_binary_accuracy: 0.5016\n",
            "Epoch 10/10\n",
            "10525/10525 [==============================] - 2477s 235ms/step - loss: 0.6934 - binary_accuracy: 0.4999 - val_loss: 0.6929 - val_binary_accuracy: 0.5016\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtfDFAnN_Neu",
        "outputId": "fa4e6337-2694-49c1-a20a-1f1e8f309e5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model\n",
        "\n",
        "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
      ],
      "metadata": {
        "id": "uBthMlTSV8kn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "loss, accuracy = classifier_model.evaluate(test_ds)\r\n",
        "\r\n",
        "print(f'Loss: {loss}')\r\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3289/3289 [==============================] - 242s 74ms/step - loss: 0.6929 - binary_accuracy: 0.4991\n",
            "Loss: 0.6929080486297607\n",
            "Accuracy: 0.49907833337783813\n"
          ]
        }
      ],
      "metadata": {
        "id": "slqB-urBV9sP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export for inference\n",
        "\n",
        "Now you just save your fine-tuned model for later use."
      ],
      "metadata": {
        "id": "Rtn7jewb6dg4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "dataset_name = 'NaverNews'\r\n",
        "saved_model_path = './saved_models/tf_bert_initial/{}_bert_multi_without'.format(dataset_name.replace('/', '_'))\r\n",
        "\r\n",
        "classifier_model.save(saved_model_path, include_optimizer=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-07-29 08:54:50.162674: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 910). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "metadata": {
        "id": "ShcvqJAgVera"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reload the model, so you can try it side by side with the model that is still in memory."
      ],
      "metadata": {
        "id": "PbI25bS1vD7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "outputs": [],
      "metadata": {
        "id": "gUEWVskZjEF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can test your model on any sentence you want, just add to the examples variable below."
      ],
      "metadata": {
        "id": "oyTappHTvNCz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "def print_my_examples(inputs, results):\r\n",
        "  result_for_printing = \\\r\n",
        "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\r\n",
        "                         for i in range(len(inputs))]\r\n",
        "  print(*result_for_printing, sep='\\n')\r\n",
        "  print()\r\n",
        "\r\n",
        "examples = [\r\n",
        "    '코로나19 확산 속 맥 못추는 코스피...카카오뱅크 상장 일정 돌입',  # this is the same sentence tried earlier\r\n",
        "    '“펀드·보험·자산관리로 금융 영역 확장” 자신감 내비친 카카오뱅크',\r\n",
        "    '[단독] 3일 내 심사한다더니…카카오뱅크 전세대출 지연에 분통',\r\n",
        "    '카카오페이 상장 9월 이후 연기',\r\n",
        "    '카카오 \"그라운드X\", 한은 CBDC 모의실험 우선협상대상자'\r\n",
        "]\r\n",
        "# examples = [\r\n",
        "#     'this is such an amazing movie!',  # this is the same sentence tried earlier\r\n",
        "#     'The movie was great!',\r\n",
        "#     'The movie was meh.',\r\n",
        "#     'The movie was okish.',\r\n",
        "#     'The movie was terrible...'\r\n",
        "# ]\r\n",
        "\r\n",
        "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\r\n",
        "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\r\n",
        "\r\n",
        "print('Results from the saved model:')\r\n",
        "print_my_examples(examples, reloaded_results)\r\n",
        "print('Results from the model in memory:')\r\n",
        "print_my_examples(examples, original_results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results from the saved model:\n",
            "input: 코로나19 확산 속 맥 못추는 코스피...카카오뱅크 상장 일정 돌입 : score: 0.507545\n",
            "input: “펀드·보험·자산관리로 금융 영역 확장” 자신감 내비친 카카오뱅크 : score: 0.507546\n",
            "input: [단독] 3일 내 심사한다더니…카카오뱅크 전세대출 지연에 분통 : score: 0.507546\n",
            "input: 카카오페이 상장 9월 이후 연기              : score: 0.507546\n",
            "input: 카카오 \"그라운드X\", 한은 CBDC 모의실험 우선협상대상자 : score: 0.507546\n",
            "\n",
            "Results from the model in memory:\n",
            "input: 코로나19 확산 속 맥 못추는 코스피...카카오뱅크 상장 일정 돌입 : score: 0.507545\n",
            "input: “펀드·보험·자산관리로 금융 영역 확장” 자신감 내비친 카카오뱅크 : score: 0.507546\n",
            "input: [단독] 3일 내 심사한다더니…카카오뱅크 전세대출 지연에 분통 : score: 0.507546\n",
            "input: 카카오페이 상장 9월 이후 연기              : score: 0.507546\n",
            "input: 카카오 \"그라운드X\", 한은 CBDC 모의실험 우선협상대상자 : score: 0.507546\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "VBWzH6exlCPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
      ],
      "metadata": {
        "id": "3cOmih754Y_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "serving_results = reloaded_model \\\n",
        "            .signatures['serving_default'](tf.constant(examples))\n",
        "\n",
        "serving_results = tf.sigmoid(serving_results['classifier'])\n",
        "\n",
        "print_my_examples(examples, serving_results)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: 코로나19 확산 속 맥 못추는 코스피...카카오뱅크 상장 일정 돌입 : score: 0.507545\n",
            "input: “펀드·보험·자산관리로 금융 영역 확장” 자신감 내비친 카카오뱅크 : score: 0.507545\n",
            "input: [단독] 3일 내 심사한다더니…카카오뱅크 전세대출 지연에 분통 : score: 0.507545\n",
            "input: 카카오페이 상장 9월 이후 연기              : score: 0.507545\n",
            "input: 카카오 \"그라운드X\", 한은 CBDC 모의실험 우선협상대상자 : score: 0.507545\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "0FdVD3973S-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next steps\n",
        "\n",
        "As a next step, you can try [Solve GLUE tasks using BERT on a TPU tutorial](https://www.tensorflow.org/text/tutorials/bert_glue), which runs on a TPU and shows you how to work with multiple inputs."
      ],
      "metadata": {
        "id": "B4gN1KwReLPN"
      }
    }
  ]
}